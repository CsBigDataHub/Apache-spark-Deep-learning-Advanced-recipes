{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Embedding\n",
    "import numpy as np\n",
    "from pickle import dump\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/test/ApacheSparkDeepLearningSolutions/Section 1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(name):\n",
    "    file = open(name, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg EBook of The jungle book, by Rudyard Kipling\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: The Jungle Book\n",
      "\n",
      "Author: Rudyard Kipling\n",
      "\n",
      "Release Date: April 30, 2011 [EBook #35997]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK THE JUNGLE BOOK ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Suzanne Shell, Ernest Schaal, and the Online\n",
      "Distributed Proofreading Team at http://www.pgdp.net (This\n",
      "file was produced from images generously made available\n",
      "by The Internet Archive/American Libraries.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                            THE JUNGLE BOOK\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    [Illustration: Rudyard Kipling]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Illustration: \"LITTLE TOOMAI LAID HIMSELF DOWN CLOSE TO THE GREAT NECK\n",
      "LEST A SWINGING BOUGH SHOULD SWEEP HIM TO THE GROUND.\" (SEE PAGE 246.)]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  THE\n",
      "                              JUNGLE BOOK\n",
      "\n",
      "\n",
      "                                   BY\n",
      "                            RUDYARD KIPLING\n",
      "\n",
      "\n",
      "                             [Illustration]\n",
      "\n",
      "\n",
      "                                NEW YORK\n",
      "                            THE CENTURY CO.\n",
      "                                  1910\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        Copyright 1893, 1894, by\n",
      "                            RUDYARD KIPLING\n",
      "                          Copyright, 1894, by\n",
      "                          HARPER and BROTHERS\n",
      "                        Copyright 1893, 1894, by\n",
      "                            THE CENTURY CO.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                CONTENTS\n",
      "\n",
      "\n",
      "                                                                PAGE\n",
      "\n",
      "    MOWGLI'S BROTHERS                                              1\n",
      "\n",
      "      HUNTING-SONG OF THE SEEONEE PACK                            42\n",
      "\n",
      "    KAA'S HUNTING                                                 47\n",
      "\n",
      "      ROAD-SONG OF THE BANDAR-LOG                                 89\n",
      "\n",
      "    \"TI\n"
     ]
    }
   ],
   "source": [
    "input_filename = 'junglebook.txt'\n",
    "doc = load_document(input_filename)\n",
    "print(doc[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def clean_document(doc):\n",
    "    doc = doc.replace('--', ' ')\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'of', 'the', 'jungle', 'book', 'by', 'rudyard', 'kipling', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'title', 'the', 'jungle', 'book', 'author', 'rudyard', 'kipling', 'release', 'date', 'april', 'ebook', 'language', 'english', 'start', 'of', 'this', 'project', 'gutenberg', 'ebook', 'the', 'jungle', 'book', 'produced', 'by', 'suzanne', 'shell', 'ernest', 'schaal', 'and', 'the', 'online', 'distributed', 'proofreading', 'team', 'at', 'httpwwwpgdpnet', 'this', 'file', 'was', 'produced', 'from', 'images', 'generously', 'made', 'available', 'by', 'the', 'internet', 'archiveamerican', 'libraries', 'the', 'jungle', 'book', 'illustration', 'rudyard', 'kipling', 'illustration', 'little', 'toomai', 'laid', 'himself', 'down', 'close', 'to', 'the', 'great', 'neck', 'lest', 'a', 'swinging', 'bough', 'should', 'sweep', 'him', 'to', 'the', 'ground', 'see', 'page', 'the', 'jungle', 'book', 'by', 'rudyard', 'kipling', 'illustration', 'new', 'york', 'the', 'century', 'co', 'copyright', 'by', 'rudyard', 'kipling', 'copyright', 'by', 'harper', 'and', 'brothers', 'copyright', 'by', 'the', 'century', 'co', 'contents', 'page', 'mowglis', 'brothers', 'huntingsong', 'of', 'the', 'seeonee', 'pack', 'kaas', 'hunting', 'roadsong', 'of', 'the', 'bandarlog', 'tiger', 'tiger', 'mowglis', 'song', 'the', 'white', 'seal', 'lukannon', 'rikkitikkitavi', 'darzees', 'chaunt', 'toomai', 'of', 'the', 'elephants', 'shiv', 'and', 'the', 'grasshopper', 'her', 'majestys', 'servants', 'paradesong', 'of', 'the', 'camp']\n",
      "Total Tokens: 55030\n",
      "Total Unique Tokens: 5358\n"
     ]
    }
   ],
   "source": [
    "tokens = clean_document(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Total Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 54979\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_document(lines, name):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(name, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = 'junglebook_sequences.txt'\n",
    "save_document(sequences, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(name):\n",
    "    file = open(name, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "input_filename = 'junglebook_sequences.txt'\n",
    "doc = load_document(input_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size : 5359\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size : %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "sequences = array(sequences)\n",
    "Input, Output = sequences[:,:-1], sequences[:,-1]\n",
    "Output = to_categorical(Output, num_classes=vocab_size)\n",
    "sequence_length = Input.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           535900    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 200)           240800    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5359)              1077159   \n",
      "=================================================================\n",
      "Total params: 2,214,859\n",
      "Trainable params: 2,214,859\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=sequence_length))\n",
    "model.add(LSTM(200, return_sequences=True))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "54979/54979 [==============================] - 441s 8ms/step - loss: 6.6568 - acc: 0.0644\n",
      "Epoch 2/75\n",
      "54979/54979 [==============================] - 394s 7ms/step - loss: 6.2480 - acc: 0.0716\n",
      "Epoch 3/75\n",
      "54979/54979 [==============================] - 393s 7ms/step - loss: 6.0628 - acc: 0.0829\n",
      "Epoch 4/75\n",
      "54979/54979 [==============================] - 426s 8ms/step - loss: 5.9071 - acc: 0.0923\n",
      "Epoch 5/75\n",
      "54979/54979 [==============================] - 444s 8ms/step - loss: 5.7676 - acc: 0.1033\n",
      "Epoch 6/75\n",
      "54979/54979 [==============================] - 453s 8ms/step - loss: 5.6557 - acc: 0.1119\n",
      "Epoch 7/75\n",
      "54979/54979 [==============================] - 440s 8ms/step - loss: 5.5582 - acc: 0.1192\n",
      "Epoch 8/75\n",
      "54979/54979 [==============================] - 440s 8ms/step - loss: 5.4694 - acc: 0.1251\n",
      "Epoch 9/75\n",
      "54979/54979 [==============================] - 416s 8ms/step - loss: 5.3493 - acc: 0.1316\n",
      "Epoch 10/75\n",
      "54979/54979 [==============================] - 412s 7ms/step - loss: 5.2543 - acc: 0.1357\n",
      "Epoch 11/75\n",
      "54979/54979 [==============================] - 410s 7ms/step - loss: 5.1705 - acc: 0.1388\n",
      "Epoch 12/75\n",
      "54979/54979 [==============================] - 414s 8ms/step - loss: 5.0992 - acc: 0.1419\n",
      "Epoch 13/75\n",
      "54979/54979 [==============================] - 428s 8ms/step - loss: 5.0292 - acc: 0.1453\n",
      "Epoch 14/75\n",
      "54979/54979 [==============================] - 441s 8ms/step - loss: 4.9612 - acc: 0.1471\n",
      "Epoch 15/75\n",
      "54979/54979 [==============================] - 425s 8ms/step - loss: 4.9177 - acc: 0.1500\n",
      "Epoch 16/75\n",
      "54979/54979 [==============================] - 435s 8ms/step - loss: 4.8732 - acc: 0.1513\n",
      "Epoch 17/75\n",
      "54979/54979 [==============================] - 440s 8ms/step - loss: 4.7849 - acc: 0.1567\n",
      "Epoch 18/75\n",
      "54979/54979 [==============================] - 428s 8ms/step - loss: 4.7188 - acc: 0.1584\n",
      "Epoch 19/75\n",
      "54979/54979 [==============================] - 437s 8ms/step - loss: 4.6674 - acc: 0.1592\n",
      "Epoch 20/75\n",
      "54979/54979 [==============================] - 359s 7ms/step - loss: 4.5979 - acc: 0.1620\n",
      "Epoch 21/75\n",
      "54979/54979 [==============================] - 266s 5ms/step - loss: 4.5290 - acc: 0.1667\n",
      "Epoch 22/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 4.4579 - acc: 0.1703\n",
      "Epoch 23/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 4.3919 - acc: 0.1719\n",
      "Epoch 24/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 4.3285 - acc: 0.1746\n",
      "Epoch 25/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 4.2609 - acc: 0.1790\n",
      "Epoch 26/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 4.1998 - acc: 0.1827\n",
      "Epoch 27/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 4.1659 - acc: 0.1869\n",
      "Epoch 28/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 4.0944 - acc: 0.1902\n",
      "Epoch 29/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 4.0661 - acc: 0.1927\n",
      "Epoch 30/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.9925 - acc: 0.1984\n",
      "Epoch 31/75\n",
      "54979/54979 [==============================] - 253s 5ms/step - loss: 3.9465 - acc: 0.2022\n",
      "Epoch 32/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.9876 - acc: 0.2015\n",
      "Epoch 33/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.9338 - acc: 0.2061\n",
      "Epoch 34/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.9082 - acc: 0.2069\n",
      "Epoch 35/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.9451 - acc: 0.2055\n",
      "Epoch 36/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.8762 - acc: 0.2106\n",
      "Epoch 37/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.7809 - acc: 0.2163\n",
      "Epoch 38/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.7249 - acc: 0.2219\n",
      "Epoch 39/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.6751 - acc: 0.2297\n",
      "Epoch 40/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.6524 - acc: 0.2307\n",
      "Epoch 41/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.6467 - acc: 0.2326\n",
      "Epoch 42/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.6267 - acc: 0.2345\n",
      "Epoch 43/75\n",
      "54979/54979 [==============================] - 253s 5ms/step - loss: 3.6238 - acc: 0.2352\n",
      "Epoch 44/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.5832 - acc: 0.2376\n",
      "Epoch 45/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.5141 - acc: 0.2458\n",
      "Epoch 46/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.4607 - acc: 0.2518\n",
      "Epoch 47/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.4075 - acc: 0.2594\n",
      "Epoch 48/75\n",
      "54979/54979 [==============================] - 253s 5ms/step - loss: 3.3562 - acc: 0.2655\n",
      "Epoch 49/75\n",
      "54979/54979 [==============================] - 254s 5ms/step - loss: 3.2843 - acc: 0.2757\n",
      "Epoch 50/75\n",
      "54979/54979 [==============================] - 254s 5ms/step - loss: 3.2427 - acc: 0.2769\n",
      "Epoch 51/75\n",
      "54979/54979 [==============================] - 254s 5ms/step - loss: 3.2034 - acc: 0.2826\n",
      "Epoch 52/75\n",
      "54979/54979 [==============================] - 253s 5ms/step - loss: 3.1831 - acc: 0.2880\n",
      "Epoch 53/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.1620 - acc: 0.2918\n",
      "Epoch 54/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.0951 - acc: 0.2982\n",
      "Epoch 55/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.0433 - acc: 0.3080\n",
      "Epoch 56/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.0288 - acc: 0.3095\n",
      "Epoch 57/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 3.0039 - acc: 0.3140\n",
      "Epoch 58/75\n",
      "54979/54979 [==============================] - 251s 5ms/step - loss: 2.9531 - acc: 0.3225\n",
      "Epoch 59/75\n",
      "54979/54979 [==============================] - 251s 5ms/step - loss: 2.9121 - acc: 0.3281\n",
      "Epoch 60/75\n",
      "54979/54979 [==============================] - 251s 5ms/step - loss: 2.8572 - acc: 0.3371\n",
      "Epoch 61/75\n",
      "54979/54979 [==============================] - 251s 5ms/step - loss: 2.8270 - acc: 0.3403\n",
      "Epoch 62/75\n",
      "54979/54979 [==============================] - 251s 5ms/step - loss: 2.7902 - acc: 0.3476\n",
      "Epoch 63/75\n",
      "54979/54979 [==============================] - 251s 5ms/step - loss: 2.7657 - acc: 0.3510\n",
      "Epoch 64/75\n",
      "54979/54979 [==============================] - 251s 5ms/step - loss: 2.7182 - acc: 0.3581\n",
      "Epoch 65/75\n",
      "54979/54979 [==============================] - 253s 5ms/step - loss: 2.6868 - acc: 0.3643\n",
      "Epoch 66/75\n",
      "54979/54979 [==============================] - 251s 5ms/step - loss: 2.6634 - acc: 0.3684\n",
      "Epoch 67/75\n",
      "54979/54979 [==============================] - 251s 5ms/step - loss: 2.6167 - acc: 0.3735\n",
      "Epoch 68/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 2.6166 - acc: 0.3776\n",
      "Epoch 69/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 2.5921 - acc: 0.3806\n",
      "Epoch 70/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 2.5532 - acc: 0.3868\n",
      "Epoch 71/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 2.5868 - acc: 0.3832\n",
      "Epoch 72/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 2.5434 - acc: 0.3914\n",
      "Epoch 73/75\n",
      "54979/54979 [==============================] - 251s 5ms/step - loss: 2.5205 - acc: 0.3970\n",
      "Epoch 74/75\n",
      "54979/54979 [==============================] - 252s 5ms/step - loss: 2.4881 - acc: 0.3990\n",
      "Epoch 75/75\n",
      "54979/54979 [==============================] - 251s 5ms/step - loss: 2.4732 - acc: 0.4072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcd2ee38b70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(Input, Output, batch_size=250, epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('junglebook_trained.h5')\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(name):\n",
    "    file = open(name, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "input_filename = 'junglebook_sequences.txt'\n",
    "doc = load_document(input_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = len(lines[0].split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('junglebook_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tens and his head began to swim outside the clearing he could hear them crashing in the undergrowth as they worked their way up the hillside but as soon as they were within the circle of the treetrunks they moved like ghosts there were whitetusked wild males with fallen leaves\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_document(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def generate_sequence(model, tokenizer, sequence_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    input_text = seed_text\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        prediction = model.predict_classes(encoded, verbose=0)\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == prediction:\n",
    "                    out_word = word\n",
    "                    break\n",
    "            input_text += ' ' + out_word\n",
    "            result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "input_filename = 'junglebook_sequences.txt'\n",
    "doc = load_document(input_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gutenbergtm electronic works to protect the project gutenbergtm concept and trademark project gutenberg is a registered trademark and may not be used if you charge for the ebooks unless you receive specific permission if you do not charge anything for copies of this ebook complying with the rules is very easy\n",
      "\n",
      "                                                                                                                                                                                                       \n"
     ]
    }
   ],
   "source": [
    "model = load_model('junglebook_trained.h5')\n",
    "\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "generated = generate_sequence(model, tokenizer, sequence_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
